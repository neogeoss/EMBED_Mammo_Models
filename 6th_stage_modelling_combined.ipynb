{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00432978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]\n",
      "*************************************\n",
      "Found 3265 files belonging to 2 classes.\n",
      "Found 1117 files belonging to 2 classes.\n",
      "Found 446 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "---time taken : 1.6214420795440674 seconds ---\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 10:45:48.951781: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 110 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 110 all-reduces with algorithm = nccl, num_packs = 1\n",
      "545/545 [==============================] - ETA: 0s - loss: 0.7165 - accuracy: 0.5210 - auc: 0.5084 - precision: 0.4745 - recall: 0.2881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 10:46:48.758387: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/545 [==============================] - 68s 76ms/step - loss: 0.7165 - accuracy: 0.5210 - auc: 0.5084 - precision: 0.4745 - recall: 0.2881 - val_loss: 0.6641 - val_accuracy: 0.6097 - val_auc: 0.6650 - val_precision: 0.7829 - val_recall: 0.2561\n",
      "Epoch 2/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.6838 - accuracy: 0.5602 - auc: 0.5605 - precision: 0.5783 - recall: 0.1971 - val_loss: 0.6877 - val_accuracy: 0.5210 - val_auc: 0.6739 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.6520 - accuracy: 0.6159 - auc: 0.6585 - precision: 0.6517 - recall: 0.3724 - val_loss: 0.7283 - val_accuracy: 0.5739 - val_auc: 0.6720 - val_precision: 0.7287 - val_recall: 0.1757\n",
      "Epoch 4/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.5841 - accuracy: 0.6855 - auc: 0.7524 - precision: 0.6566 - recall: 0.6770 - val_loss: 0.6453 - val_accuracy: 0.6670 - val_auc: 0.7437 - val_precision: 0.6973 - val_recall: 0.5383\n",
      "Epoch 5/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.5444 - accuracy: 0.7152 - auc: 0.7943 - precision: 0.6871 - recall: 0.7106 - val_loss: 0.7010 - val_accuracy: 0.6625 - val_auc: 0.7546 - val_precision: 0.7337 - val_recall: 0.4636\n",
      "Epoch 6/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.5203 - accuracy: 0.7292 - auc: 0.8141 - precision: 0.7038 - recall: 0.7205 - val_loss: 0.6753 - val_accuracy: 0.6902 - val_auc: 0.7642 - val_precision: 0.7299 - val_recall: 0.5607\n",
      "Epoch 7/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4871 - accuracy: 0.7538 - auc: 0.8418 - precision: 0.7313 - recall: 0.7429 - val_loss: 0.7313 - val_accuracy: 0.6759 - val_auc: 0.7622 - val_precision: 0.7344 - val_recall: 0.5065\n",
      "Epoch 8/20\n",
      "545/545 [==============================] - 38s 71ms/step - loss: 0.4736 - accuracy: 0.7605 - auc: 0.8504 - precision: 0.7465 - recall: 0.7337 - val_loss: 0.6876 - val_accuracy: 0.6885 - val_auc: 0.7632 - val_precision: 0.7169 - val_recall: 0.5776\n",
      "Epoch 9/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.4422 - accuracy: 0.7874 - auc: 0.8730 - precision: 0.7663 - recall: 0.7805 - val_loss: 0.6728 - val_accuracy: 0.6983 - val_auc: 0.7689 - val_precision: 0.7097 - val_recall: 0.6262\n",
      "Epoch 10/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.4397 - accuracy: 0.7825 - auc: 0.8740 - precision: 0.7575 - recall: 0.7825 - val_loss: 0.6662 - val_accuracy: 0.6947 - val_auc: 0.7680 - val_precision: 0.7046 - val_recall: 0.6243\n",
      "Epoch 11/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4362 - accuracy: 0.7856 - auc: 0.8762 - precision: 0.7658 - recall: 0.7759 - val_loss: 0.6754 - val_accuracy: 0.6911 - val_auc: 0.7698 - val_precision: 0.6947 - val_recall: 0.6336\n",
      "Epoch 12/20\n",
      "545/545 [==============================] - 39s 71ms/step - loss: 0.4248 - accuracy: 0.7991 - auc: 0.8834 - precision: 0.7857 - recall: 0.7805 - val_loss: 0.6684 - val_accuracy: 0.7037 - val_auc: 0.7703 - val_precision: 0.6984 - val_recall: 0.6710\n",
      "Epoch 13/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4281 - accuracy: 0.7936 - auc: 0.8809 - precision: 0.7718 - recall: 0.7891 - val_loss: 0.6637 - val_accuracy: 0.7055 - val_auc: 0.7686 - val_precision: 0.7004 - val_recall: 0.6729\n",
      "Epoch 14/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4211 - accuracy: 0.8006 - auc: 0.8863 - precision: 0.7815 - recall: 0.7924 - val_loss: 0.6697 - val_accuracy: 0.7037 - val_auc: 0.7696 - val_precision: 0.6984 - val_recall: 0.6710\n",
      "Epoch 15/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4216 - accuracy: 0.8031 - auc: 0.8855 - precision: 0.7864 - recall: 0.7910 - val_loss: 0.6642 - val_accuracy: 0.6974 - val_auc: 0.7708 - val_precision: 0.6966 - val_recall: 0.6523\n",
      "Epoch 16/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4117 - accuracy: 0.8037 - auc: 0.8906 - precision: 0.7844 - recall: 0.7963 - val_loss: 0.6687 - val_accuracy: 0.6947 - val_auc: 0.7721 - val_precision: 0.6940 - val_recall: 0.6486\n",
      "Epoch 17/20\n",
      "545/545 [==============================] - 38s 70ms/step - loss: 0.4197 - accuracy: 0.8003 - auc: 0.8857 - precision: 0.7781 - recall: 0.7976 - val_loss: 0.6737 - val_accuracy: 0.6983 - val_auc: 0.7699 - val_precision: 0.6996 - val_recall: 0.6486\n",
      "---time taken : 686.5522539615631 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 10:57:13.882511: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 2s 24ms/step - loss: 0.6680 - accuracy: 0.7108 - auc: 0.7907 - precision: 0.6692 - recall: 0.8018\n",
      "Test accuracy : 0.7107623219490051\n",
      "Test AUC : 0.7906746864318848\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest AUC :\u001b[39m\u001b[38;5;124m'\u001b[39m, testauc)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# F1 = 2*float(precision)*float(recall)/(float(precision) + float(recall))\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest F1 :\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mF1\u001b[49m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest precision :\u001b[39m\u001b[38;5;124m'\u001b[39m, precision)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest recall :\u001b[39m\u001b[38;5;124m'\u001b[39m, recall)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F1' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\" # Choose which GPUs by checking current use with nvidia-smi\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "## Keras library also provides ResNet101V2 and ResNet50V2. Import them and use it for other experiments. \n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import metrics\n",
    "import time\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "# Check CUDA functionality, restart kernel to change GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"*************************************\")\n",
    "print(gpus)\n",
    "print(\"*************************************\")\n",
    "\n",
    "# Define function to preprocess images as required by ResNet\n",
    "def preprocess(images, labels):\n",
    "    return tf.keras.applications.resnet_v2.preprocess_input(images), labels\n",
    "\n",
    "\n",
    "#setup train, validation, and test folders\n",
    "traindir = './train_800_600_combined_comparison_birad'\n",
    "valdir = './val_800_600_combined_comparison_birad'\n",
    "testdir = './test_800_600_combined_comparison_birad'\n",
    "dirName = '800_600'\n",
    "\n",
    "\n",
    "buffersize = 3\n",
    "#im_dim = 512\n",
    "im_dim_x = 800\n",
    "im_dim_y = 600\n",
    "batchSizeIntInitial = 10 \n",
    "batchSizeInt = 6\n",
    "\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    traindir, image_size=(im_dim_x, im_dim_y), batch_size=batchSizeInt)\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valdir, image_size=(im_dim_x, im_dim_y), batch_size=batchSizeInt)\n",
    "test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    testdir, image_size=(im_dim_x, im_dim_y), batch_size=batchSizeInt)\n",
    "\n",
    "test_ds = test.map(preprocess)\n",
    "train_ds = train.map(preprocess)\n",
    "val_ds = val.map(preprocess)\n",
    "train_ds = train_ds.prefetch(buffer_size=buffersize)\n",
    "val_ds = val_ds.prefetch(buffer_size=buffersize)\n",
    "\n",
    "\n",
    "## set up hyperparameters, such as epochs, learning rates, cutoffs.\n",
    "## Smaller Lr is prefferred when datasets are mixed\n",
    "epochs = 20\n",
    "lr = 0.002\n",
    "cutoff=0.5\n",
    "start_time = time.time()\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope(): # the entire model needs to be compiled within the scope of the distribution strategy\n",
    "    cb1 = EarlyStopping(monitor='val_accuracy', patience=4) # define early stopping callback function\n",
    "    cb2 = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=0.00001) # define LR reduction callback function\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    metr = [metrics.BinaryAccuracy(name='accuracy', threshold=cutoff), metrics.AUC(name='auc'), metrics.Precision(name='precision'),\n",
    "                metrics.Recall(name='recall')]\n",
    "#                 tfa.metrics.F1Score(name='f1_score')]\n",
    "    ptmodel = ResNet50V2(include_top=False, weights='imagenet', classes=2, input_shape=(im_dim_x, im_dim_y, 3), pooling='avg') # compile resnet152v2 with imagenet weights\n",
    "    ptmodel.trainable = False # freeze layers\n",
    "    ptmodel.layers[-1].trainable == True\n",
    "\n",
    "    # un-freeze the BatchNorm layers\n",
    "    for layer in ptmodel.layers:\n",
    "        if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "            layer.trainable = True\n",
    "\n",
    "    last_output = ptmodel.output\n",
    "    x = tf.keras.layers.Flatten()(last_output)\n",
    "    x = tf.keras.layers.Dense(2048, activation='relu')(x)\n",
    "#     x = tf.keras.layers.Dropout(0.15)(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "#    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    # # x = tf.keras.layers.Dropout(0.5, seed=34)(x)\n",
    "    # x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    model = tf.keras.Model(ptmodel.input, x)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='BinaryCrossentropy',\n",
    "                  metrics=metr)\n",
    "\n",
    "print(\"---time taken : %s seconds ---\" % (time.time() - start_time))\n",
    "# Train model\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=[cb1, cb2])\n",
    "print(\"---time taken : %s seconds ---\" % (time.time() - start_time))\n",
    "# Test model\n",
    "testloss, testaccuracy, testauc, precision, recall = model.evaluate(test_ds)\n",
    "print('Test accuracy :', testaccuracy)\n",
    "print('Test AUC :', testauc)\n",
    "\n",
    "# F1 = 2*float(precision)*float(recall)/(float(precision) + float(recall))\n",
    "# print('Test F1 :', F1)\n",
    "print('Test precision :', precision)\n",
    "print('Test recall :', recall)\n",
    "\n",
    "# Model path setup. \n",
    "if not os.path.exists(\"saved_model_resnet50_combined\"+dirName):\n",
    "    os.makedirs(\"saved_model_resnet50_combined\"+dirName+'/')\n",
    "    \n",
    "model.save('saved_model_resnet50_combined'+dirName+'/resnet152v2_1')\n",
    "predicted_probs = np.array([])\n",
    "true_classes =  np.array([])\n",
    "IterationChecker = 0\n",
    "for images, labels in test_ds:\n",
    "    if IterationChecker == 0:\n",
    "        predicted_probs = model(images)\n",
    "        true_classes = labels.numpy()\n",
    "\n",
    "    IterationChecker += 1\n",
    "\n",
    "    predicted_probs = np.concatenate([predicted_probs,\n",
    "                       model(images)])\n",
    "    true_classes = np.concatenate([true_classes, labels.numpy()])\n",
    "# Since they are sigmoid outputs, you need to transform them into classes with a threshold, i.e 0.5 here:\n",
    "predicted_classes = [1 * (x[0]>=cutoff) for x in predicted_probs]\n",
    "# confusion matrix etc:\n",
    "conf_matrix = tf.math.confusion_matrix(true_classes, predicted_classes)\n",
    "print(conf_matrix)\n",
    "\n",
    "predicted_probs=np.squeeze(predicted_probs)\n",
    "predicted_classes = np.array(predicted_classes)\n",
    "true_classes=np.squeeze(true_classes)\n",
    "summedResults = np.stack((predicted_probs,predicted_classes,true_classes), axis = 1)\n",
    "##Print out statistics which test files are correctly predicted or not. \n",
    "np.savetxt(\"Resnet50_combined_comp_EMBED.csv\", summedResults, delimiter=',', header=\"predicted_probabilty,predicted_classes,true_classes\", comments=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acd6af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
